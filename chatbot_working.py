# -*- coding: utf-8 -*-
"""ChatBot Working.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JAgKUyzJSup-OK6KskmFDKLnbzvXFPvV
"""

!pip install gradio
!pip install faiss-cpu
!pip install huggingface_hub gradio transformers sentence-transformers faiss-cpu
!pip install faiss-cpu gradio python-docx sentence-transformers transformers --quiet
!pip install faiss-cpu gradio python-docx sentence-transformers transformers
!pip install transformers

!python --version

import re
import numpy as np
import faiss
import gradio as gr
from docx import Document
from google.colab import files
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline

# Upload .docx file
uploaded = files.upload()
filename = list(uploaded.keys())[0]

def read_docx(file_path):
    doc = Document(file_path)
    return "\n".join([para.text for para in doc.paragraphs if para.text.strip()])

doc_text = read_docx(filename)

# Split text into chunks
def split_text(text, max_length=500):
    sentences = re.split(r'(?<=[.?!])\s+', text)
    chunks, current_chunk = [], ""
    for sentence in sentences:
        if len(current_chunk) + len(sentence) < max_length:
            current_chunk += " " + sentence
        else:
            chunks.append(current_chunk.strip())
            current_chunk = sentence
    if current_chunk:
        chunks.append(current_chunk.strip())
    return chunks

chunks = split_text(doc_text)

# Create FAISS index
embedder = SentenceTransformer('all-MiniLM-L6-v2')
embeddings = embedder.encode(chunks, convert_to_numpy=True)
dimension = embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(embeddings)

# Load model
tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-base")
model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-base")
llm = pipeline("text2text-generation", model=model, tokenizer=tokenizer)

# Q&A function
def generate_answer(question):
    if not question.strip():
        return "⚠️ Please enter a question."
    try:
        question_embedding = embedder.encode([question])
        D, I = index.search(np.array(question_embedding), k=3)
        if len(I[0]) == 0:
            return "No relevant information found."
        top_chunks = "\n\n".join([chunks[i] for i in I[0]])
        prompt = f"""Context: {top_chunks}\nQuestion: {question}\nAnswer:"""
        response = llm(prompt)[0]['generated_text']
        return response
    except Exception as e:
        import traceback
        return "❌ Error:\n" + traceback.format_exc()

# Launch Gradio app with compact layout
with gr.Blocks(css="""
    .container {
        max-width: 600px;
        margin: 0 auto;
        padding: 20px;
    }
    .title {
        text-align: center;
        font-size: 28px;
        font-weight: bold;
        color: #367588;
        margin-bottom: 20px;
    }
    #send_btn, #clear_btn {
        background-color: #367588 !important;
        color: white !important;
    }
    #question_input_box label,
    #answer_output_box label {
        font-weight: bold;
        font-size: 18px;
        color: red; /* You can change this if needed */
    }
    .gr-textbox textarea {
        font-size: 18px;
        font-weight: bold;
    }
""") as demo:
    with gr.Column(elem_classes=["container"]):
        gr.Markdown("<div class='title'>Tenant Analyzer Chatbot</div>")

        question_input = gr.Textbox(
            label="Ask a question",
            placeholder="Type your question here...",
            lines=1,
            elem_id="question_input_box"
        )

        answer_output = gr.Textbox(
            label="Answer",
            lines=8,
            elem_id="answer_output_box"
        )

        with gr.Row(equal_height=True):
            send_btn = gr.Button("Send", elem_id="send_btn")
            clear_btn = gr.Button("Clear", elem_id="clear_btn")

        send_btn.click(fn=generate_answer, inputs=question_input, outputs=answer_output)
        clear_btn.click(lambda: ("", ""), inputs=None, outputs=[question_input, answer_output])

demo.launch(share=True)